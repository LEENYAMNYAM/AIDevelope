{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674b8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97754ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2119, 0.5761, 0.2119])\n"
     ]
    }
   ],
   "source": [
    "z = torch.FloatTensor([3,4,3])\n",
    "y_hat = F.softmax(z, dim=0)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984750cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9884, 0.1221, 0.4612, 0.1514, 0.1027],\n",
      "        [0.4120, 0.4914, 0.1037, 0.2252, 0.3585],\n",
      "        [0.1528, 0.3426, 0.1447, 0.0615, 0.8389]], requires_grad=True)\n",
      "tensor([1.8258, 1.5908, 1.5406], grad_fn=<SumBackward1>)\n",
      "tensor([[0.3501, 0.1472, 0.2067, 0.1516, 0.1444],\n",
      "        [0.2176, 0.2356, 0.1599, 0.1806, 0.2063],\n",
      "        [0.1639, 0.1982, 0.1626, 0.1496, 0.3256]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 1., 1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3,5,requires_grad=True)\n",
    "print(z)\n",
    "print(z.sum(dim=1))\n",
    "y_hat = F.softmax(z, dim=1)\n",
    "print(y_hat)\n",
    "print(y_hat.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9619634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 2])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[2],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 5미만인 랜덤한 정수를 첫번째 차원이 3개인 행렬(리스트)로 만들어줌\n",
    "y= torch.randint(5, (3,)).long()\n",
    "print(y)\n",
    "y_one_hot = torch.zeros_like(y_hat)\n",
    "print(y_one_hot)\n",
    "# unsqueeze : 차원을 하나 높여줌\n",
    "print(y.unsqueeze(1))\n",
    "\n",
    "# 파라미터 설명(dim, index, value)\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1), 1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57ab5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abc8b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x_data=[[1,2,1,1,],\n",
    "        [2,1,3,2], \n",
    "        [3,1,3,4],\n",
    "        [4,1,5,5],\n",
    "        [1,7,5,5],\n",
    "        [1,2,5,6],\n",
    "        [1,6,6,6],\n",
    "        [1,7,7,7]]\n",
    "y_data=[2,2,2,1,1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5543c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.LongTensor(y_data)\n",
    "print(x_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53b0d161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8,3)\n",
    "print(y_one_hot)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaffe393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True)\n",
      "tensor([[0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros((1,3), requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c7c5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([w,b], lr=0.1)\n",
    "epochs = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e38528a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 1.0986123085021973\n",
      "epoch : 100, cost : 0.7041995525360107\n",
      "epoch : 200, cost : 0.6229996085166931\n",
      "epoch : 300, cost : 0.5657169222831726\n",
      "epoch : 400, cost : 0.5152913331985474\n",
      "epoch : 500, cost : 0.46766167879104614\n",
      "epoch : 600, cost : 0.4212779700756073\n",
      "epoch : 700, cost : 0.3754016160964966\n",
      "epoch : 800, cost : 0.3297654986381531\n",
      "epoch : 900, cost : 0.2850721478462219\n",
      "epoch : 1000, cost : 0.2481546849012375\n",
      "epoch : 1100, cost : 0.23267611861228943\n",
      "epoch : 1200, cost : 0.22139869630336761\n",
      "epoch : 1300, cost : 0.21112915873527527\n",
      "epoch : 1400, cost : 0.20173649489879608\n",
      "epoch : 1500, cost : 0.1931133270263672\n",
      "epoch : 1600, cost : 0.18516966700553894\n",
      "epoch : 1700, cost : 0.1778293251991272\n",
      "epoch : 1800, cost : 0.17102724313735962\n",
      "epoch : 1900, cost : 0.16470739245414734\n",
      "epoch : 2000, cost : 0.15882128477096558\n",
      "epoch : 2100, cost : 0.15332657098770142\n",
      "epoch : 2200, cost : 0.14818626642227173\n",
      "epoch : 2300, cost : 0.14336799085140228\n",
      "epoch : 2400, cost : 0.1388428956270218\n",
      "epoch : 2500, cost : 0.13458555936813354\n",
      "epoch : 2600, cost : 0.13057351112365723\n",
      "epoch : 2700, cost : 0.12678645551204681\n",
      "epoch : 2800, cost : 0.12320639193058014\n",
      "epoch : 2900, cost : 0.1198171004652977\n",
      "epoch : 3000, cost : 0.11660401523113251\n",
      "epoch : 3100, cost : 0.11355391144752502\n",
      "epoch : 3200, cost : 0.11065512150526047\n",
      "epoch : 3300, cost : 0.10789669305086136\n",
      "epoch : 3400, cost : 0.10526886582374573\n",
      "epoch : 3500, cost : 0.10276280343532562\n",
      "epoch : 3600, cost : 0.10037018358707428\n",
      "epoch : 3700, cost : 0.09808389842510223\n",
      "epoch : 3800, cost : 0.09589694440364838\n",
      "epoch : 3900, cost : 0.0938030406832695\n",
      "epoch : 4000, cost : 0.09179650247097015\n",
      "epoch : 4100, cost : 0.08987218141555786\n",
      "epoch : 4200, cost : 0.0880250483751297\n",
      "epoch : 4300, cost : 0.08625059574842453\n",
      "epoch : 4400, cost : 0.08454479277133942\n",
      "epoch : 4500, cost : 0.08290377259254456\n",
      "epoch : 4600, cost : 0.08132387697696686\n",
      "epoch : 4700, cost : 0.07980184257030487\n",
      "epoch : 4800, cost : 0.07833458483219147\n",
      "epoch : 4900, cost : 0.07691922038793564\n",
      "epoch : 5000, cost : 0.07555314153432846\n",
      "epoch : 5100, cost : 0.07423384487628937\n",
      "epoch : 5200, cost : 0.07295893132686615\n",
      "epoch : 5300, cost : 0.07172621786594391\n",
      "epoch : 5400, cost : 0.0705336406826973\n",
      "epoch : 5500, cost : 0.06937950849533081\n",
      "epoch : 5600, cost : 0.06826186180114746\n",
      "epoch : 5700, cost : 0.06717892736196518\n",
      "epoch : 5800, cost : 0.06612920761108398\n",
      "epoch : 5900, cost : 0.06511135399341583\n",
      "epoch : 6000, cost : 0.06412370502948761\n",
      "epoch : 6100, cost : 0.06316513568162918\n",
      "epoch : 6200, cost : 0.06223434954881668\n",
      "epoch : 6300, cost : 0.06133010610938072\n",
      "epoch : 6400, cost : 0.06045131757855415\n",
      "epoch : 6500, cost : 0.05959698185324669\n",
      "epoch : 6600, cost : 0.05876614898443222\n",
      "epoch : 6700, cost : 0.057957716286182404\n",
      "epoch : 6800, cost : 0.05717100948095322\n",
      "epoch : 6900, cost : 0.05640484765172005\n",
      "epoch : 7000, cost : 0.055658817291259766\n",
      "epoch : 7100, cost : 0.054931916296482086\n",
      "epoch : 7200, cost : 0.054223500192165375\n",
      "epoch : 7300, cost : 0.05353277921676636\n",
      "epoch : 7400, cost : 0.052859317511320114\n",
      "epoch : 7500, cost : 0.052202221006155014\n",
      "epoch : 7600, cost : 0.0515611357986927\n",
      "epoch : 7700, cost : 0.05093526840209961\n",
      "epoch : 7800, cost : 0.05032427981495857\n",
      "epoch : 7900, cost : 0.049727585166692734\n",
      "epoch : 8000, cost : 0.04914465174078941\n",
      "epoch : 8100, cost : 0.04857509210705757\n",
      "epoch : 8200, cost : 0.04801838472485542\n",
      "epoch : 8300, cost : 0.04747416824102402\n",
      "epoch : 8400, cost : 0.04694193974137306\n",
      "epoch : 8500, cost : 0.046421267092227936\n",
      "epoch : 8600, cost : 0.04591197520494461\n",
      "epoch : 8700, cost : 0.045413583517074585\n",
      "epoch : 8800, cost : 0.04492576792836189\n",
      "epoch : 8900, cost : 0.04444824904203415\n",
      "epoch : 9000, cost : 0.04398060962557793\n",
      "epoch : 9100, cost : 0.04352254047989845\n",
      "epoch : 9200, cost : 0.043073851615190506\n",
      "epoch : 9300, cost : 0.042634181678295135\n",
      "epoch : 9400, cost : 0.04220336675643921\n",
      "epoch : 9500, cost : 0.0417809821665287\n",
      "epoch : 9600, cost : 0.04136691242456436\n",
      "epoch : 9700, cost : 0.04096090421080589\n",
      "epoch : 9800, cost : 0.04056260362267494\n",
      "epoch : 9900, cost : 0.04017195850610733\n",
      "epoch : 10000, cost : 0.039788685739040375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    y_hat = F.softmax(x_train.matmul(w)+b, dim=1)\n",
    "    cost = (y_one_hot*-torch.log(y_hat)).sum(dim=1).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100==0 :\n",
    "        print(f'epoch : {epoch}, cost : {cost.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cf21fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.9508e-11, 6.5690e-06, 9.9999e-01],\n",
      "        [1.7811e-04, 1.9474e-02, 9.8035e-01],\n",
      "        [1.1670e-13, 4.1809e-02, 9.5819e-01],\n",
      "        [6.1379e-10, 9.6248e-01, 3.7520e-02],\n",
      "        [7.2622e-02, 9.2453e-01, 2.8516e-03],\n",
      "        [3.8537e-02, 9.6146e-01, 7.0804e-08],\n",
      "        [9.1230e-01, 8.7705e-02, 8.1008e-08],\n",
      "        [9.9212e-01, 7.8797e-03, 5.7659e-11]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_hat = F.softmax(x_train.matmul(w) + b, dim=1)\n",
    "print(y_hat)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2873620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for y in y_hat:\n",
    "    print(y.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3901186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(4,3)  #input_dim = 4, output_dim=3\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "epochs=10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12a2601a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 1.6167852878570557\n",
      "epoch : 100, cost : 0.6588909029960632\n",
      "epoch : 200, cost : 0.5734434127807617\n",
      "epoch : 300, cost : 0.5181514024734497\n",
      "epoch : 400, cost : 0.4732654392719269\n",
      "epoch : 500, cost : 0.4335160553455353\n",
      "epoch : 600, cost : 0.39656299352645874\n",
      "epoch : 700, cost : 0.3609141707420349\n",
      "epoch : 800, cost : 0.3253922760486603\n",
      "epoch : 900, cost : 0.2891782224178314\n",
      "epoch : 1000, cost : 0.254148006439209\n",
      "epoch : 1100, cost : 0.23497344553470612\n",
      "epoch : 1200, cost : 0.2234926074743271\n",
      "epoch : 1300, cost : 0.21305321156978607\n",
      "epoch : 1400, cost : 0.20350952446460724\n",
      "epoch : 1500, cost : 0.1947513222694397\n",
      "epoch : 1600, cost : 0.1866869032382965\n",
      "epoch : 1700, cost : 0.17923785746097565\n",
      "epoch : 1800, cost : 0.17233797907829285\n",
      "epoch : 1900, cost : 0.1659296602010727\n",
      "epoch : 2000, cost : 0.15996332466602325\n",
      "epoch : 2100, cost : 0.1543959230184555\n",
      "epoch : 2200, cost : 0.149189293384552\n",
      "epoch : 2300, cost : 0.14431047439575195\n",
      "epoch : 2400, cost : 0.1397300809621811\n",
      "epoch : 2500, cost : 0.13542188704013824\n",
      "epoch : 2600, cost : 0.13136310875415802\n",
      "epoch : 2700, cost : 0.12753304839134216\n",
      "epoch : 2800, cost : 0.12391342222690582\n",
      "epoch : 2900, cost : 0.12048744410276413\n",
      "epoch : 3000, cost : 0.11724031716585159\n",
      "epoch : 3100, cost : 0.11415884643793106\n",
      "epoch : 3200, cost : 0.11123076826334\n",
      "epoch : 3300, cost : 0.10844492167234421\n",
      "epoch : 3400, cost : 0.10579176992177963\n",
      "epoch : 3500, cost : 0.10326193273067474\n",
      "epoch : 3600, cost : 0.10084725171327591\n",
      "epoch : 3700, cost : 0.09854014217853546\n",
      "epoch : 3800, cost : 0.09633376449346542\n",
      "epoch : 3900, cost : 0.09422154724597931\n",
      "epoch : 4000, cost : 0.09219776839017868\n",
      "epoch : 4100, cost : 0.09025724232196808\n",
      "epoch : 4200, cost : 0.08839491009712219\n",
      "epoch : 4300, cost : 0.0866062119603157\n",
      "epoch : 4400, cost : 0.08488684892654419\n",
      "epoch : 4500, cost : 0.08323289453983307\n",
      "epoch : 4600, cost : 0.08164099603891373\n",
      "epoch : 4700, cost : 0.08010751008987427\n",
      "epoch : 4800, cost : 0.07862940430641174\n",
      "epoch : 4900, cost : 0.0772036537528038\n",
      "epoch : 5000, cost : 0.07582785934209824\n",
      "epoch : 5100, cost : 0.07449933886528015\n",
      "epoch : 5200, cost : 0.07321544736623764\n",
      "epoch : 5300, cost : 0.07197438925504684\n",
      "epoch : 5400, cost : 0.07077394425868988\n",
      "epoch : 5500, cost : 0.06961197406053543\n",
      "epoch : 5600, cost : 0.06848704069852829\n",
      "epoch : 5700, cost : 0.06739728897809982\n",
      "epoch : 5800, cost : 0.06634093821048737\n",
      "epoch : 5900, cost : 0.06531664729118347\n",
      "epoch : 6000, cost : 0.06432289630174637\n",
      "epoch : 6100, cost : 0.06335856020450592\n",
      "epoch : 6200, cost : 0.06242216005921364\n",
      "epoch : 6300, cost : 0.061512645334005356\n",
      "epoch : 6400, cost : 0.060628753155469894\n",
      "epoch : 6500, cost : 0.059769485145807266\n",
      "epoch : 6600, cost : 0.05893395096063614\n",
      "epoch : 6700, cost : 0.05812101438641548\n",
      "epoch : 6800, cost : 0.0573299415409565\n",
      "epoch : 6900, cost : 0.056559670716524124\n",
      "epoch : 7000, cost : 0.05580959841609001\n",
      "epoch : 7100, cost : 0.05507880076766014\n",
      "epoch : 7200, cost : 0.05436661094427109\n",
      "epoch : 7300, cost : 0.053672343492507935\n",
      "epoch : 7400, cost : 0.05299541354179382\n",
      "epoch : 7500, cost : 0.05233491584658623\n",
      "epoch : 7600, cost : 0.05169064551591873\n",
      "epoch : 7700, cost : 0.05106183513998985\n",
      "epoch : 7800, cost : 0.05044781416654587\n",
      "epoch : 7900, cost : 0.04984819144010544\n",
      "epoch : 8000, cost : 0.049262505024671555\n",
      "epoch : 8100, cost : 0.04869021475315094\n",
      "epoch : 8200, cost : 0.04813097417354584\n",
      "epoch : 8300, cost : 0.047584183514118195\n",
      "epoch : 8400, cost : 0.047049593180418015\n",
      "epoch : 8500, cost : 0.04652661457657814\n",
      "epoch : 8600, cost : 0.04601500555872917\n",
      "epoch : 8700, cost : 0.045514389872550964\n",
      "epoch : 8800, cost : 0.04502447322010994\n",
      "epoch : 8900, cost : 0.04454483091831207\n",
      "epoch : 9000, cost : 0.044075168669223785\n",
      "epoch : 9100, cost : 0.04361523315310478\n",
      "epoch : 9200, cost : 0.04316459968686104\n",
      "epoch : 9300, cost : 0.04272311553359032\n",
      "epoch : 9400, cost : 0.042290449142456055\n",
      "epoch : 9500, cost : 0.04186642915010452\n",
      "epoch : 9600, cost : 0.0414506159722805\n",
      "epoch : 9700, cost : 0.04104302078485489\n",
      "epoch : 9800, cost : 0.04064320772886276\n",
      "epoch : 9900, cost : 0.04025102034211159\n",
      "epoch : 10000, cost : 0.03986618295311928\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    y_hat = model(x_train)\n",
    "    cost = F.cross_entropy(y_hat, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f'epoch : {epoch}, cost : {cost.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4dd5e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-11.4190,  -0.1089,  11.8240],\n",
      "        [ -4.3004,   0.4547,   4.3716],\n",
      "        [-18.3870,   8.3334,  11.4634],\n",
      "        [-12.6073,   8.7538,   5.5110],\n",
      "        [  0.9640,   3.5059,  -2.2739],\n",
      "        [  4.2154,   7.4301,  -9.0255],\n",
      "        [  7.0720,   4.7319,  -9.1734],\n",
      "        [ 10.5025,   5.6714, -13.0728]], grad_fn=<AddmmBackward0>)\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "preds=model(x_train)\n",
    "print(preds)\n",
    "\n",
    "for pred in preds:\n",
    "    print(pred.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9935cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16a1d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "epochs = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52b2adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eopch : 0, cost: 2.6837055683135986\n",
      "eopch : 100, cost: 0.6966274976730347\n",
      "eopch : 200, cost: 0.6156858205795288\n",
      "eopch : 300, cost: 0.5584627985954285\n",
      "eopch : 400, cost: 0.5081623792648315\n",
      "eopch : 500, cost: 0.46063682436943054\n",
      "eopch : 600, cost: 0.41430869698524475\n",
      "eopch : 700, cost: 0.36844050884246826\n",
      "eopch : 800, cost: 0.3228070139884949\n",
      "eopch : 900, cost: 0.27836117148399353\n",
      "eopch : 1000, cost: 0.24378979206085205\n",
      "eopch : 1100, cost: 0.22987177968025208\n",
      "eopch : 1200, cost: 0.21881912648677826\n",
      "eopch : 1300, cost: 0.20874805748462677\n",
      "eopch : 1400, cost: 0.1995321661233902\n",
      "eopch : 1500, cost: 0.19106729328632355\n",
      "eopch : 1600, cost: 0.18326592445373535\n",
      "eopch : 1700, cost: 0.1760539710521698\n",
      "eopch : 1800, cost: 0.16936816275119781\n",
      "eopch : 1900, cost: 0.16315393149852753\n",
      "eopch : 2000, cost: 0.1573638617992401\n",
      "eopch : 2100, cost: 0.1519566774368286\n",
      "eopch : 2200, cost: 0.14689670503139496\n",
      "eopch : 2300, cost: 0.14215199649333954\n",
      "eopch : 2400, cost: 0.13769464194774628\n",
      "eopch : 2500, cost: 0.13349953293800354\n",
      "eopch : 2600, cost: 0.1295449137687683\n",
      "eopch : 2700, cost: 0.12581101059913635\n",
      "eopch : 2800, cost: 0.12228014320135117\n",
      "eopch : 2900, cost: 0.11893652379512787\n",
      "eopch : 3000, cost: 0.11576583236455917\n",
      "eopch : 3100, cost: 0.11275536566972733\n",
      "eopch : 3200, cost: 0.10989324003458023\n",
      "eopch : 3300, cost: 0.1071692481637001\n",
      "eopch : 3400, cost: 0.10457346588373184\n",
      "eopch : 3500, cost: 0.10209741443395615\n",
      "eopch : 3600, cost: 0.09973311424255371\n",
      "eopch : 3700, cost: 0.09747330099344254\n",
      "eopch : 3800, cost: 0.09531109780073166\n",
      "eopch : 3900, cost: 0.09324062615633011\n",
      "eopch : 4000, cost: 0.09125616401433945\n",
      "eopch : 4100, cost: 0.08935241401195526\n",
      "eopch : 4200, cost: 0.08752503991127014\n",
      "eopch : 4300, cost: 0.08576910942792892\n",
      "eopch : 4400, cost: 0.0840807557106018\n",
      "eopch : 4500, cost: 0.08245635777711868\n",
      "eopch : 4600, cost: 0.08089219778776169\n",
      "eopch : 4700, cost: 0.07938506454229355\n",
      "eopch : 4800, cost: 0.07793205231428146\n",
      "eopch : 4900, cost: 0.07653015106916428\n",
      "eopch : 5000, cost: 0.0751768946647644\n",
      "eopch : 5100, cost: 0.07386975735425949\n",
      "eopch : 5200, cost: 0.07260625809431076\n",
      "eopch : 5300, cost: 0.07138475775718689\n",
      "eopch : 5400, cost: 0.07020276039838791\n",
      "eopch : 5500, cost: 0.06905877590179443\n",
      "eopch : 5600, cost: 0.06795067340135574\n",
      "eopch : 5700, cost: 0.06687700748443604\n",
      "eopch : 5800, cost: 0.06583599746227264\n",
      "eopch : 5900, cost: 0.06482651084661484\n",
      "eopch : 6000, cost: 0.06384697556495667\n",
      "eopch : 6100, cost: 0.06289618462324142\n",
      "eopch : 6200, cost: 0.06197277083992958\n",
      "eopch : 6300, cost: 0.06107567995786667\n",
      "eopch : 6400, cost: 0.06020372733473778\n",
      "eopch : 6500, cost: 0.05935594439506531\n",
      "eopch : 6600, cost: 0.05853133276104927\n",
      "eopch : 6700, cost: 0.057729002088308334\n",
      "eopch : 6800, cost: 0.05694800615310669\n",
      "eopch : 6900, cost: 0.05618758872151375\n",
      "eopch : 7000, cost: 0.05544687807559967\n",
      "eopch : 7100, cost: 0.054725151509046555\n",
      "eopch : 7200, cost: 0.05402178317308426\n",
      "eopch : 7300, cost: 0.05333584174513817\n",
      "eopch : 7400, cost: 0.05266707390546799\n",
      "eopch : 7500, cost: 0.05201442539691925\n",
      "eopch : 7600, cost: 0.05137772858142853\n",
      "eopch : 7700, cost: 0.05075599625706673\n",
      "eopch : 7800, cost: 0.05014907568693161\n",
      "eopch : 7900, cost: 0.049556292593479156\n",
      "eopch : 8000, cost: 0.048977162688970566\n",
      "eopch : 8100, cost: 0.048411231487989426\n",
      "eopch : 8200, cost: 0.04785808175802231\n",
      "eopch : 8300, cost: 0.04731728136539459\n",
      "eopch : 8400, cost: 0.046788327395915985\n",
      "eopch : 8500, cost: 0.046270981431007385\n",
      "eopch : 8600, cost: 0.045764897018671036\n",
      "eopch : 8700, cost: 0.04526947811245918\n",
      "eopch : 8800, cost: 0.0447845384478569\n",
      "eopch : 8900, cost: 0.0443098358809948\n",
      "eopch : 9000, cost: 0.04384486377239227\n",
      "eopch : 9100, cost: 0.043389592319726944\n",
      "eopch : 9200, cost: 0.04294348135590553\n",
      "eopch : 9300, cost: 0.0425063893198967\n",
      "eopch : 9400, cost: 0.04207790270447731\n",
      "eopch : 9500, cost: 0.04165801405906677\n",
      "eopch : 9600, cost: 0.041246190667152405\n",
      "eopch : 9700, cost: 0.040842410176992416\n",
      "eopch : 9800, cost: 0.04044632613658905\n",
      "eopch : 9900, cost: 0.04005776345729828\n",
      "eopch : 10000, cost: 0.03967655822634697\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    y_hat = model(x_train)\n",
    "    cost = F.cross_entropy(y_hat, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f'eopch : {epoch}, cost: {cost.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47bf1968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0856e+01,  4.5740e-01,  1.2281e+01],\n",
      "        [-4.6509e+00, -1.7458e-02,  3.9047e+00],\n",
      "        [-1.8929e+01,  7.6294e+00,  1.0764e+01],\n",
      "        [-1.3611e+01,  7.4681e+00,  4.2205e+00],\n",
      "        [ 2.1079e+00,  4.6548e+00, -1.1292e+00],\n",
      "        [ 2.9079e+00,  6.1276e+00, -9.5868e+00],\n",
      "        [ 7.3436e+00,  4.9986e+00, -8.6052e+00],\n",
      "        [ 1.0806e+01,  5.9695e+00, -1.2416e+01]], grad_fn=<AddmmBackward0>)\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "predictions = model(x_train)\n",
    "print(predictions)\n",
    "for pred in predictions:\n",
    "    print(pred.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6996d221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "iris = datasets.load_iris()\n",
    "iris.data\n",
    "iris.target\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce71ee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 4])\n",
      "torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor(iris.data)\n",
    "y_train = torch.LongTensor(iris.target)\n",
    "print(x_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcada69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # cross_entropy()를 사용하면 맨 마지막 단의 softmax()는 생략 가능 \n",
    "        return self.linear(x)      \n",
    "        # return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a617b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "epochs=10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c8c03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eopch : 0, cost: 1.5569689273834229\n",
      "eopch : 100, cost: 0.4369971752166748\n",
      "eopch : 200, cost: 0.25845861434936523\n",
      "eopch : 300, cost: 0.21572257578372955\n",
      "eopch : 400, cost: 0.1886710673570633\n",
      "eopch : 500, cost: 0.1699049323797226\n",
      "eopch : 600, cost: 0.15607570111751556\n",
      "eopch : 700, cost: 0.1454285979270935\n",
      "eopch : 800, cost: 0.13695544004440308\n",
      "eopch : 900, cost: 0.1300354301929474\n",
      "eopch : 1000, cost: 0.12426526099443436\n",
      "eopch : 1100, cost: 0.11937137693166733\n",
      "eopch : 1200, cost: 0.11516137421131134\n",
      "eopch : 1300, cost: 0.11149604618549347\n",
      "eopch : 1400, cost: 0.1082719936966896\n",
      "eopch : 1500, cost: 0.10541076958179474\n",
      "eopch : 1600, cost: 0.10285170376300812\n",
      "eopch : 1700, cost: 0.1005471795797348\n",
      "eopch : 1800, cost: 0.09845922142267227\n",
      "eopch : 1900, cost: 0.0965571478009224\n",
      "eopch : 2000, cost: 0.09481591731309891\n",
      "eopch : 2100, cost: 0.09321492165327072\n",
      "eopch : 2200, cost: 0.09173690527677536\n",
      "eopch : 2300, cost: 0.09036743640899658\n",
      "eopch : 2400, cost: 0.08909427374601364\n",
      "eopch : 2500, cost: 0.08790698647499084\n",
      "eopch : 2600, cost: 0.08679666370153427\n",
      "eopch : 2700, cost: 0.08575554192066193\n",
      "eopch : 2800, cost: 0.08477696776390076\n",
      "eopch : 2900, cost: 0.0838550478219986\n",
      "eopch : 3000, cost: 0.08298470079898834\n",
      "eopch : 3100, cost: 0.08216136693954468\n",
      "eopch : 3200, cost: 0.08138108253479004\n",
      "eopch : 3300, cost: 0.08064034581184387\n",
      "eopch : 3400, cost: 0.0799359530210495\n",
      "eopch : 3500, cost: 0.07926508784294128\n",
      "eopch : 3600, cost: 0.07862523943185806\n",
      "eopch : 3700, cost: 0.07801413536071777\n",
      "eopch : 3800, cost: 0.07742971181869507\n",
      "eopch : 3900, cost: 0.07687012106180191\n",
      "eopch : 4000, cost: 0.0763336643576622\n",
      "eopch : 4100, cost: 0.07581883668899536\n",
      "eopch : 4200, cost: 0.07532420754432678\n",
      "eopch : 4300, cost: 0.07484850287437439\n",
      "eopch : 4400, cost: 0.07439060509204865\n",
      "eopch : 4500, cost: 0.07394935935735703\n",
      "eopch : 4600, cost: 0.07352384924888611\n",
      "eopch : 4700, cost: 0.07311316579580307\n",
      "eopch : 4800, cost: 0.07271645963191986\n",
      "eopch : 4900, cost: 0.0723329484462738\n",
      "eopch : 5000, cost: 0.0719619169831276\n",
      "eopch : 5100, cost: 0.0716027021408081\n",
      "eopch : 5200, cost: 0.07125469297170639\n",
      "eopch : 5300, cost: 0.07091734558343887\n",
      "eopch : 5400, cost: 0.0705900713801384\n",
      "eopch : 5500, cost: 0.07027240842580795\n",
      "eopch : 5600, cost: 0.06996389478445053\n",
      "eopch : 5700, cost: 0.0696641132235527\n",
      "eopch : 5800, cost: 0.06937261670827866\n",
      "eopch : 5900, cost: 0.06908905506134033\n",
      "eopch : 6000, cost: 0.06881304830312729\n",
      "eopch : 6100, cost: 0.06854427605867386\n",
      "eopch : 6200, cost: 0.06828244775533676\n",
      "eopch : 6300, cost: 0.06802722066640854\n",
      "eopch : 6400, cost: 0.06777834892272949\n",
      "eopch : 6500, cost: 0.06753554940223694\n",
      "eopch : 6600, cost: 0.06729858368635178\n",
      "eopch : 6700, cost: 0.0670672282576561\n",
      "eopch : 6800, cost: 0.0668412521481514\n",
      "eopch : 6900, cost: 0.06662044674158096\n",
      "eopch : 7000, cost: 0.06640461087226868\n",
      "eopch : 7100, cost: 0.06619356572628021\n",
      "eopch : 7200, cost: 0.06598713248968124\n",
      "eopch : 7300, cost: 0.06578512489795685\n",
      "eopch : 7400, cost: 0.06558738648891449\n",
      "eopch : 7500, cost: 0.06539378315210342\n",
      "eopch : 7600, cost: 0.06520417332649231\n",
      "eopch : 7700, cost: 0.06501840054988861\n",
      "eopch : 7800, cost: 0.06483630836009979\n",
      "eopch : 7900, cost: 0.06465782225131989\n",
      "eopch : 8000, cost: 0.06448279321193695\n",
      "eopch : 8100, cost: 0.06431112438440323\n",
      "eopch : 8200, cost: 0.06414268910884857\n",
      "eopch : 8300, cost: 0.06397739797830582\n",
      "eopch : 8400, cost: 0.06381513923406601\n",
      "eopch : 8500, cost: 0.06365585327148438\n",
      "eopch : 8600, cost: 0.06349939107894897\n",
      "eopch : 8700, cost: 0.06334570050239563\n",
      "eopch : 8800, cost: 0.06319470703601837\n",
      "eopch : 8900, cost: 0.06304629147052765\n",
      "eopch : 9000, cost: 0.06290042400360107\n",
      "eopch : 9100, cost: 0.0627569928765297\n",
      "eopch : 9200, cost: 0.06261596083641052\n",
      "eopch : 9300, cost: 0.062477219849824905\n",
      "eopch : 9400, cost: 0.06234074756503105\n",
      "eopch : 9500, cost: 0.0622064583003521\n",
      "eopch : 9600, cost: 0.06207431107759476\n",
      "eopch : 9700, cost: 0.06194419786334038\n",
      "eopch : 9800, cost: 0.06181611493229866\n",
      "eopch : 9900, cost: 0.061690013855695724\n",
      "eopch : 10000, cost: 0.06156580522656441\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    y_hat = model(x_train)\n",
    "    cost = F.cross_entropy(y_hat, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print(f'eopch : {epoch}, cost: {cost.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "047f4ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4543e+01,  6.8317e+00, -2.2963e+01],\n",
      "        [ 1.2351e+01,  6.6374e+00, -2.0392e+01],\n",
      "        [ 1.3276e+01,  6.4941e+00, -2.1195e+01],\n",
      "        [ 1.1657e+01,  6.2848e+00, -1.9512e+01],\n",
      "        [ 1.4737e+01,  6.7322e+00, -2.3112e+01],\n",
      "        [ 1.4498e+01,  6.4348e+00, -2.2809e+01],\n",
      "        [ 1.3086e+01,  6.0765e+00, -2.0717e+01],\n",
      "        [ 1.3462e+01,  6.6779e+00, -2.1786e+01],\n",
      "        [ 1.1108e+01,  6.1441e+00, -1.8676e+01],\n",
      "        [ 1.2396e+01,  6.8430e+00, -2.0826e+01],\n",
      "        [ 1.5267e+01,  7.0711e+00, -2.4061e+01],\n",
      "        [ 1.2574e+01,  6.4248e+00, -2.0758e+01],\n",
      "        [ 1.2391e+01,  6.8005e+00, -2.0660e+01],\n",
      "        [ 1.3131e+01,  6.4734e+00, -2.0872e+01],\n",
      "        [ 1.8686e+01,  7.6311e+00, -2.7852e+01],\n",
      "        [ 1.7941e+01,  6.8391e+00, -2.6653e+01],\n",
      "        [ 1.6650e+01,  6.6571e+00, -2.4832e+01],\n",
      "        [ 1.4328e+01,  6.5697e+00, -2.2433e+01],\n",
      "        [ 1.4869e+01,  6.9937e+00, -2.3712e+01],\n",
      "        [ 1.4896e+01,  6.5122e+00, -2.3157e+01],\n",
      "        [ 1.3085e+01,  6.9619e+00, -2.1820e+01],\n",
      "        [ 1.4312e+01,  6.2509e+00, -2.2217e+01],\n",
      "        [ 1.6190e+01,  6.5595e+00, -2.4090e+01],\n",
      "        [ 1.1547e+01,  5.8804e+00, -1.9036e+01],\n",
      "        [ 1.0961e+01,  6.2580e+00, -1.9241e+01],\n",
      "        [ 1.1450e+01,  6.6249e+00, -1.9641e+01],\n",
      "        [ 1.2494e+01,  6.0985e+00, -2.0221e+01],\n",
      "        [ 1.4180e+01,  6.8749e+00, -2.2719e+01],\n",
      "        [ 1.4349e+01,  6.9311e+00, -2.2815e+01],\n",
      "        [ 1.1662e+01,  6.3273e+00, -1.9677e+01],\n",
      "        [ 1.1469e+01,  6.4267e+00, -1.9529e+01],\n",
      "        [ 1.3731e+01,  6.5492e+00, -2.1771e+01],\n",
      "        [ 1.6607e+01,  7.1329e+00, -2.5708e+01],\n",
      "        [ 1.7823e+01,  7.2222e+00, -2.6877e+01],\n",
      "        [ 1.2181e+01,  6.5811e+00, -2.0296e+01],\n",
      "        [ 1.4338e+01,  6.8460e+00, -2.2484e+01],\n",
      "        [ 1.5780e+01,  7.2824e+00, -2.4514e+01],\n",
      "        [ 1.4777e+01,  6.8954e+00, -2.3381e+01],\n",
      "        [ 1.2015e+01,  6.1991e+00, -1.9591e+01],\n",
      "        [ 1.3637e+01,  6.7767e+00, -2.2048e+01],\n",
      "        [ 1.4691e+01,  6.5266e+00, -2.2678e+01],\n",
      "        [ 9.3940e+00,  6.0404e+00, -1.6454e+01],\n",
      "        [ 1.2752e+01,  6.1978e+00, -2.0411e+01],\n",
      "        [ 1.2432e+01,  5.5740e+00, -1.9570e+01],\n",
      "        [ 1.2529e+01,  6.0279e+00, -2.0604e+01],\n",
      "        [ 1.1961e+01,  6.2766e+00, -1.9600e+01],\n",
      "        [ 1.4573e+01,  6.7185e+00, -2.3181e+01],\n",
      "        [ 1.2563e+01,  6.3397e+00, -2.0428e+01],\n",
      "        [ 1.5092e+01,  6.9723e+00, -2.3799e+01],\n",
      "        [ 1.3631e+01,  6.7342e+00, -2.1882e+01],\n",
      "        [-3.5731e+00,  3.7326e+00, -3.6451e+00],\n",
      "        [-3.7611e+00,  2.9892e+00, -2.5596e+00],\n",
      "        [-5.4074e+00,  3.2614e+00, -1.4325e+00],\n",
      "        [-5.5321e+00,  2.9079e+00, -1.0921e-01],\n",
      "        [-5.5987e+00,  3.0350e+00, -6.7553e-01],\n",
      "        [-6.0290e+00,  2.8242e+00, -1.5200e-01],\n",
      "        [-4.8582e+00,  2.5167e+00, -1.1667e+00],\n",
      "        [-1.8017e+00,  3.4895e+00, -4.0825e+00],\n",
      "        [-4.6251e+00,  3.6570e+00, -2.4066e+00],\n",
      "        [-4.2592e+00,  2.4026e+00, -9.4087e-01],\n",
      "        [-4.1772e+00,  3.4797e+00, -1.6927e+00],\n",
      "        [-3.7586e+00,  2.6634e+00, -1.9513e+00],\n",
      "        [-4.3816e+00,  4.1882e+00, -2.5953e+00],\n",
      "        [-6.2521e+00,  2.8456e+00, -6.4970e-02],\n",
      "        [-9.9403e-01,  3.2251e+00, -4.8526e+00],\n",
      "        [-2.8524e+00,  3.6037e+00, -3.9690e+00],\n",
      "        [-5.8967e+00,  2.2003e+00,  3.4952e-01],\n",
      "        [-3.4260e+00,  3.9318e+00, -3.6165e+00],\n",
      "        [-7.7968e+00,  2.7981e+00,  2.0614e+00],\n",
      "        [-3.6521e+00,  3.5848e+00, -2.7559e+00],\n",
      "        [-6.8940e+00,  1.5428e+00,  1.8537e+00],\n",
      "        [-2.6403e+00,  3.4973e+00, -3.7255e+00],\n",
      "        [-8.6678e+00,  2.6726e+00,  2.5937e+00],\n",
      "        [-6.1906e+00,  3.3701e+00, -7.1517e-01],\n",
      "        [-3.3610e+00,  3.6262e+00, -3.4016e+00],\n",
      "        [-3.3958e+00,  3.5056e+00, -3.2980e+00],\n",
      "        [-5.9350e+00,  3.4821e+00, -9.7753e-01],\n",
      "        [-7.0936e+00,  2.4850e+00,  1.0656e+00],\n",
      "        [-5.5661e+00,  2.5961e+00, -2.8531e-01],\n",
      "        [-7.4198e-01,  4.1673e+00, -5.9801e+00],\n",
      "        [-3.6576e+00,  3.5423e+00, -2.5907e+00],\n",
      "        [-2.9046e+00,  3.8598e+00, -3.6265e+00],\n",
      "        [-2.7803e+00,  3.5191e+00, -3.5680e+00],\n",
      "        [-9.7458e+00,  2.0019e+00,  4.0991e+00],\n",
      "        [-6.2463e+00,  2.0027e+00,  8.7186e-01],\n",
      "        [-3.9381e+00,  2.3309e+00, -1.8046e+00],\n",
      "        [-4.6812e+00,  3.1750e+00, -1.9217e+00],\n",
      "        [-6.2854e+00,  3.4757e+00, -1.7550e-01],\n",
      "        [-3.3149e+00,  2.9465e+00, -2.7336e+00],\n",
      "        [-4.7949e+00,  2.9066e+00, -9.2893e-01],\n",
      "        [-6.3628e+00,  2.9455e+00,  1.5426e-01],\n",
      "        [-5.3456e+00,  2.9005e+00, -9.8060e-01],\n",
      "        [-3.6868e+00,  3.4642e+00, -2.6523e+00],\n",
      "        [-1.9955e+00,  3.5889e+00, -3.9338e+00],\n",
      "        [-4.9587e+00,  2.8929e+00, -9.9828e-01],\n",
      "        [-3.4630e+00,  3.2516e+00, -3.0191e+00],\n",
      "        [-4.0467e+00,  2.9904e+00, -2.0792e+00],\n",
      "        [-3.7106e+00,  3.4287e+00, -2.8792e+00],\n",
      "        [ 3.1511e-01,  3.5913e+00, -6.0020e+00],\n",
      "        [-3.8774e+00,  3.0466e+00, -2.1751e+00],\n",
      "        [-1.3787e+01, -5.6341e-01,  1.0179e+01],\n",
      "        [-1.0741e+01,  1.0185e+00,  6.2115e+00],\n",
      "        [-1.2096e+01,  1.3321e+00,  6.6929e+00],\n",
      "        [-1.1604e+01,  1.4951e+00,  6.0847e+00],\n",
      "        [-1.2822e+01,  5.3306e-01,  8.2842e+00],\n",
      "        [-1.4987e+01,  1.4368e+00,  8.9275e+00],\n",
      "        [-9.3934e+00,  9.8823e-01,  5.2871e+00],\n",
      "        [-1.3621e+01,  2.0937e+00,  7.0134e+00],\n",
      "        [-1.3455e+01,  1.7816e+00,  7.6910e+00],\n",
      "        [-1.1645e+01,  2.6805e-01,  7.1043e+00],\n",
      "        [-7.8891e+00,  1.4448e+00,  2.8640e+00],\n",
      "        [-1.0768e+01,  1.5000e+00,  5.6560e+00],\n",
      "        [-1.0469e+01,  1.2581e+00,  5.4533e+00],\n",
      "        [-1.1330e+01,  7.1469e-01,  7.3166e+00],\n",
      "        [-1.1447e+01, -2.9179e-01,  8.4518e+00],\n",
      "        [-9.7849e+00,  4.4903e-01,  5.7268e+00],\n",
      "        [-1.0348e+01,  1.7476e+00,  4.6468e+00],\n",
      "        [-1.2616e+01,  1.2129e+00,  6.4232e+00],\n",
      "        [-1.8331e+01,  8.4752e-01,  1.2883e+01],\n",
      "        [-1.0836e+01,  2.3226e+00,  5.1126e+00],\n",
      "        [-1.1063e+01,  7.2054e-01,  6.4441e+00],\n",
      "        [-9.8608e+00,  6.6956e-01,  5.8425e+00],\n",
      "        [-1.5872e+01,  1.7432e+00,  9.4617e+00],\n",
      "        [-8.5758e+00,  1.8855e+00,  3.3641e+00],\n",
      "        [-1.0613e+01,  1.0462e+00,  5.4965e+00],\n",
      "        [-1.1077e+01,  2.1597e+00,  4.5277e+00],\n",
      "        [-7.8441e+00,  1.8417e+00,  2.7096e+00],\n",
      "        [-7.8195e+00,  1.6860e+00,  2.6568e+00],\n",
      "        [-1.2443e+01,  8.0870e-01,  7.8235e+00],\n",
      "        [-1.0308e+01,  2.7961e+00,  3.2758e+00],\n",
      "        [-1.2954e+01,  2.0424e+00,  6.6806e+00],\n",
      "        [-1.0223e+01,  2.1011e+00,  3.3235e+00],\n",
      "        [-1.2658e+01,  5.4677e-01,  8.3535e+00],\n",
      "        [-8.6378e+00,  2.5595e+00,  2.3757e+00],\n",
      "        [-1.2199e+01,  2.3472e+00,  5.7165e+00],\n",
      "        [-1.2553e+01,  1.2897e+00,  7.1975e+00],\n",
      "        [-1.1051e+01, -7.9756e-02,  7.2156e+00],\n",
      "        [-1.0154e+01,  1.6481e+00,  4.4981e+00],\n",
      "        [-7.4564e+00,  1.6428e+00,  2.4122e+00],\n",
      "        [-9.3873e+00,  1.4118e+00,  4.2765e+00],\n",
      "        [-1.1458e+01,  3.1730e-01,  7.4005e+00],\n",
      "        [-8.2037e+00,  1.0547e+00,  3.8193e+00],\n",
      "        [-1.0741e+01,  1.0185e+00,  6.2115e+00],\n",
      "        [-1.2313e+01,  5.1058e-01,  7.7168e+00],\n",
      "        [-1.1474e+01, -1.5230e-03,  7.6166e+00],\n",
      "        [-9.4598e+00,  8.0225e-01,  5.2573e+00],\n",
      "        [-1.0066e+01,  1.5693e+00,  5.2196e+00],\n",
      "        [-9.1642e+00,  1.3905e+00,  4.1895e+00],\n",
      "        [-9.9351e+00,  1.9458e-01,  5.9352e+00],\n",
      "        [-9.2449e+00,  1.3773e+00,  4.1907e+00]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred=model(x_train)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aef5623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9800)\n"
     ]
    }
   ],
   "source": [
    "accuracys = []\n",
    "for i in range(len(pred)):\n",
    "    accuracys.append(y_train[i]==pred[i].argmax())\n",
    "\n",
    "print(torch.FloatTensor(accuracys).sum()/len(accuracys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38b58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
