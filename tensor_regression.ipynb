{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "130eed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32a50aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cc7d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[1,2], [2,3], [3,4], [4,5], [5,6]], dtype=np.float32)\n",
    "y_train = np.array([[3], [5], [7], [9], [11]],  dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ccfb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.w = tf.Variable(tf.random.normal([2, 1]), name='weight', dtype=tf.float32)\n",
    "        self.b = tf.Variable(tf.random.normal([1]), name='bias', dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e27e402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'a:0' shape=() dtype=float32, numpy=10.0>\n",
      "<tf.Variable 'b:0' shape=() dtype=float32, numpy=20.0>\n",
      "tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(10.0, name='a')\n",
    "b = tf.Variable(20.0, name='b')\n",
    "print(a)\n",
    "print(b)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77587e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loos_fn(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    return tf.reduce_mean(tf.square(y - y_pred)) # Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58d93999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 202.07211303710938, Weights: [0.5285529  0.07555723], Bias: [-0.2819097]\n",
      "Epoch 100, Loss: 0.0035919328220188618, Weights: [1.1298343  0.90957224], Bias: [-0.04917621]\n",
      "Epoch 200, Loss: 0.0016026556259021163, Weights: [1.1056604  0.92066175], Bias: [-0.01391278]\n",
      "Epoch 300, Loss: 0.0007150806486606598, Weights: [1.0895128 0.9280695], Bias: [0.00964206]\n",
      "Epoch 400, Loss: 0.00031904541538096964, Weights: [1.0787271 0.9330174], Bias: [0.02537589]\n",
      "Epoch 500, Loss: 0.00014235595881473273, Weights: [1.0715226  0.93632233], Bias: [0.03588559]\n",
      "Epoch 600, Loss: 6.351657066261396e-05, Weights: [1.06671    0.93853015], Bias: [0.04290578]\n",
      "Epoch 700, Loss: 2.8338443371467292e-05, Weights: [1.063495  0.9400051], Bias: [0.04759497]\n",
      "Epoch 800, Loss: 1.2644212802115362e-05, Weights: [1.0613477  0.94099027], Bias: [0.05072713]\n",
      "Epoch 900, Loss: 5.6411527111777104e-06, Weights: [1.0599134 0.9416483], Bias: [0.05281926]\n",
      "Epoch 1000, Loss: 2.516193490009755e-06, Weights: [1.0589554 0.9420878], Bias: [0.05421669]\n",
      "Epoch 1100, Loss: 1.1232084489165572e-06, Weights: [1.0583155  0.94238126], Bias: [0.05515011]\n",
      "Epoch 1200, Loss: 5.013424697608571e-07, Weights: [1.0578885  0.94257706], Bias: [0.05577371]\n",
      "Epoch 1300, Loss: 2.235139220374549e-07, Weights: [1.057603 0.942708], Bias: [0.05619036]\n",
      "Epoch 1400, Loss: 9.99680338509279e-08, Weights: [1.0574125 0.9427953], Bias: [0.05646859]\n",
      "Epoch 1500, Loss: 4.466202341291137e-08, Weights: [1.0572851 0.9428536], Bias: [0.05665455]\n",
      "Epoch 1600, Loss: 1.9911066573286007e-08, Weights: [1.0572001 0.9428926], Bias: [0.05677874]\n",
      "Epoch 1700, Loss: 8.898314085570291e-09, Weights: [1.0571436 0.9429185], Bias: [0.05686174]\n",
      "Epoch 1800, Loss: 3.978141371874244e-09, Weights: [1.0571054 0.942936 ], Bias: [0.0569173]\n",
      "Epoch 1900, Loss: 1.7801085050450638e-09, Weights: [1.0570801 0.9429475], Bias: [0.05695438]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # 자동 미분을 수행하기 위해 제공되는 자동 그라디언트 recoding 기능을 사용합니다.\n",
    "    # tf.GradientTape()는 자동 미분을 위한 컨텍스트 매니저입니다.\n",
    "    # 이 컨텍스트 매니저 안에서 연산을 수행하면, 그라디언트를 계산할 수 있습니다.\n",
    "    # tape는 연산을 기록하고, 나중에 그라디언트를 계산할 수 있게 합니다.\n",
    "    with tf.GradientTape() as tape:    \n",
    "        loss = loos_fn(model, x_train, y_train)\n",
    "\n",
    "        gradients = tape.gradient(loss, [model.w, model.b])\n",
    "        optimizer.apply_gradients(zip(gradients, [model.w, model.b]))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.numpy()}, Weights: {model.w.numpy().flatten()}, Bias: {model.b.numpy().flatten()}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b51136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.000047 15.000066 17.000082]\n"
     ]
    }
   ],
   "source": [
    "pred = model(np.array([[6,7], [7,8], [8,9]], dtype=np.float32))\n",
    "print(pred.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2efd543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87</td>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>88</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>96</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>81</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>86</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>82</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>89</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1    2    3\n",
       "0   73  80   75  152\n",
       "1   93  88   93  185\n",
       "2   89  91   90  180\n",
       "3   96  98  100  196\n",
       "4   73  66   70  142\n",
       "5   53  46   55  101\n",
       "6   69  74   77  149\n",
       "7   47  56   60  115\n",
       "8   87  79   90  175\n",
       "9   79  70   88  164\n",
       "10  69  70   73  141\n",
       "11  70  65   74  141\n",
       "12  93  95   91  184\n",
       "13  79  80   73  152\n",
       "14  70  73   78  148\n",
       "15  93  89   96  192\n",
       "16  78  75   68  147\n",
       "17  81  90   93  183\n",
       "18  88  92   86  177\n",
       "19  78  83   77  159\n",
       "20  82  86   90  177\n",
       "21  86  82   89  175\n",
       "22  78  83   85  175\n",
       "23  76  83   71  149\n",
       "24  96  93   95  192"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/data-01-test-score.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e23adab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]]\n",
      "[[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(df.values[:,:-1], dtype=np.float32)\n",
    "print(x_train)\n",
    "y_train = np.array(df.values[:,-1].reshape(-1, 1), dtype=np.float32)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "63602edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.w = tf.Variable(tf.random.normal([3, 1]), name='weight', dtype=tf.float32)\n",
    "        self.b = tf.Variable(tf.random.normal([1]), name='bias', dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bca8493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loos_fn(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    return tf.reduce_mean(tf.square(y - y_pred)) # Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b0e725bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3896.11376953125, Weights: [ 0.550462    1.4222746  -0.37521136], Bias: [-1.2999083]\n",
      "Epoch 100, Loss: 66.99120330810547, Weights: [ 0.693824    1.5283172  -0.16084917], Bias: [-1.2977343]\n",
      "Epoch 200, Loss: 61.843780517578125, Weights: [ 0.6842014   1.4823879  -0.10661794], Bias: [-1.2974764]\n",
      "Epoch 300, Loss: 57.131893157958984, Weights: [ 0.6748723   1.4385221  -0.05468828], Bias: [-1.2972332]\n",
      "Epoch 400, Loss: 52.81867218017578, Weights: [ 0.66582674  1.3966289  -0.00496163], Bias: [-1.2970045]\n",
      "Epoch 500, Loss: 48.87028884887695, Weights: [0.6570556  1.3566204  0.04265668], Bias: [-1.2967917]\n",
      "Epoch 600, Loss: 45.2557373046875, Weights: [0.64855    1.3184133  0.08825701], Bias: [-1.2965922]\n",
      "Epoch 700, Loss: 41.94679641723633, Weights: [0.640301  1.2819276 0.1319256], Bias: [-1.2964058]\n",
      "Epoch 800, Loss: 38.91754150390625, Weights: [0.6323001  1.2470876  0.17374521], Bias: [-1.2962317]\n",
      "Epoch 900, Loss: 36.14426803588867, Weights: [0.6245393  1.21382    0.21379499], Bias: [-1.2960697]\n",
      "Epoch 1000, Loss: 33.605255126953125, Weights: [0.6170113  1.1820544  0.25215098], Bias: [-1.2959192]\n",
      "Epoch 1100, Loss: 31.280683517456055, Weights: [0.60970795 1.1517249  0.28888524], Bias: [-1.2957796]\n",
      "Epoch 1200, Loss: 29.152393341064453, Weights: [0.6026222  1.1227677  0.32406735], Bias: [-1.2956502]\n",
      "Epoch 1300, Loss: 27.203697204589844, Weights: [0.5957469  1.0951217  0.35776368], Bias: [-1.295531]\n",
      "Epoch 1400, Loss: 25.4194278717041, Weights: [0.5890761  1.0687273  0.39003816], Bias: [-1.2954232]\n",
      "Epoch 1500, Loss: 23.785627365112305, Weights: [0.5826033 1.043529  0.4209516], Bias: [-1.2953241]\n",
      "Epoch 1600, Loss: 22.289588928222656, Weights: [0.5763211  1.0194751  0.45056155], Bias: [-1.2952329]\n",
      "Epoch 1700, Loss: 20.9196834564209, Weights: [0.57022405 0.996514   0.47892386], Bias: [-1.2951494]\n",
      "Epoch 1800, Loss: 19.665185928344727, Weights: [0.5643058 0.9745974 0.5060918], Bias: [-1.2950771]\n",
      "Epoch 1900, Loss: 18.51634979248047, Weights: [0.55856097 0.95367837 0.53211653], Bias: [-1.2950107]\n",
      "Epoch 2000, Loss: 17.464265823364258, Weights: [0.552984  0.9337126 0.5570465], Bias: [-1.2949511]\n",
      "Epoch 2100, Loss: 16.500696182250977, Weights: [0.5475699  0.9146574  0.58092856], Bias: [-1.2949011]\n",
      "Epoch 2200, Loss: 15.618197441101074, Weights: [0.5423136  0.89647216 0.60380715], Bias: [-1.2948544]\n",
      "Epoch 2300, Loss: 14.809910774230957, Weights: [0.53720987 0.879118   0.62572527], Bias: [-1.2948186]\n",
      "Epoch 2400, Loss: 14.069558143615723, Weights: [0.53225404 0.8625578  0.64672405], Bias: [-1.2947854]\n",
      "Epoch 2500, Loss: 13.391399383544922, Weights: [0.52744156 0.8467563  0.6668423 ], Bias: [-1.2947615]\n",
      "Epoch 2600, Loss: 12.770177841186523, Weights: [0.52276796 0.8316789  0.68611777], Bias: [-1.2947397]\n",
      "Epoch 2700, Loss: 12.201091766357422, Weights: [0.51822877 0.8172939  0.70458615], Bias: [-1.2947278]\n",
      "Epoch 2800, Loss: 11.6797513961792, Weights: [0.5138202 0.8035701 0.7222818], Bias: [-1.2947159]\n",
      "Epoch 2900, Loss: 11.202105522155762, Weights: [0.50953805 0.7904774  0.7392374 ], Bias: [-1.2947153]\n",
      "Epoch 3000, Loss: 10.764473915100098, Weights: [0.50537866 0.7779876  0.7554849 ], Bias: [-1.2947153]\n",
      "Epoch 3100, Loss: 10.363500595092773, Weights: [0.50133795 0.7660742  0.771054  ], Bias: [-1.294721]\n",
      "Epoch 3200, Loss: 9.996077537536621, Weights: [0.4974121  0.75471115 0.78597367], Bias: [-1.2947329]\n",
      "Epoch 3300, Loss: 9.659355163574219, Weights: [0.49359775 0.7438732  0.80027163], Bias: [-1.2947448]\n",
      "Epoch 3400, Loss: 9.350774765014648, Weights: [0.4898915 0.7335374 0.8139742], Bias: [-1.2947655]\n",
      "Epoch 3500, Loss: 9.067986488342285, Weights: [0.4862902  0.72368073 0.8271063 ], Bias: [-1.2947893]\n",
      "Epoch 3600, Loss: 8.808804512023926, Weights: [0.48279065 0.714282   0.83969223], Bias: [-1.2948132]\n",
      "Epoch 3700, Loss: 8.571215629577637, Weights: [0.47939   0.7053203 0.851755 ], Bias: [-1.2948443]\n",
      "Epoch 3800, Loss: 8.353436470031738, Weights: [0.476085   0.6967759  0.86331695], Bias: [-1.29488]\n",
      "Epoch 3900, Loss: 8.15377140045166, Weights: [0.47287288 0.6886301  0.8743994 ], Bias: [-1.2949158]\n",
      "Epoch 4000, Loss: 7.970746040344238, Weights: [0.46975085 0.680865   0.88502216], Bias: [-1.2949518]\n",
      "Epoch 4100, Loss: 7.802912712097168, Weights: [0.46671608 0.6734632  0.89520514], Bias: [-1.2949995]\n",
      "Epoch 4200, Loss: 7.649038314819336, Weights: [0.4637661 0.6664085 0.9049666], Bias: [-1.2950472]\n",
      "Epoch 4300, Loss: 7.5079169273376465, Weights: [0.46089834 0.6596851  0.9143244 ], Bias: [-1.2950948]\n",
      "Epoch 4400, Loss: 7.3785014152526855, Weights: [0.45811033 0.65327764 0.9232959 ], Bias: [-1.2951425]\n",
      "Epoch 4500, Loss: 7.259803295135498, Weights: [0.45539984 0.6471723  0.93189704], Bias: [-1.2951996]\n",
      "Epoch 4600, Loss: 7.1509175300598145, Weights: [0.45276454 0.64135486 0.94014376], Bias: [-1.2952592]\n",
      "Epoch 4700, Loss: 7.051025867462158, Weights: [0.4502023 0.6358125 0.9480506], Bias: [-1.2953188]\n",
      "Epoch 4800, Loss: 6.9593987464904785, Weights: [0.44771072 0.630533   0.95563203], Bias: [-1.2953784]\n",
      "Epoch 4900, Loss: 6.875311851501465, Weights: [0.44528767 0.6255041  0.9629019 ], Bias: [-1.295438]\n",
      "Epoch 5000, Loss: 6.798147678375244, Weights: [0.4429313 0.6207144 0.9698732], Bias: [-1.2955053]\n",
      "Epoch 5100, Loss: 6.7273430824279785, Weights: [0.44063988 0.61615294 0.97655857], Bias: [-1.2955768]\n",
      "Epoch 5200, Loss: 6.6623406410217285, Weights: [0.43841127 0.6118095  0.98296976], Bias: [-1.2956483]\n",
      "Epoch 5300, Loss: 6.6026930809021, Weights: [0.43624374 0.60767376 0.9891186 ], Bias: [-1.2957199]\n",
      "Epoch 5400, Loss: 6.547910213470459, Weights: [0.4341355  0.60373634 0.99501604], Bias: [-1.2957914]\n",
      "Epoch 5500, Loss: 6.497620105743408, Weights: [0.43208474 0.59998864 1.0006723 ], Bias: [-1.2958629]\n",
      "Epoch 5600, Loss: 6.451443672180176, Weights: [0.43009004 0.5964217  1.0060973 ], Bias: [-1.295935]\n",
      "Epoch 5700, Loss: 6.409021377563477, Weights: [0.42814982 0.5930269  1.0113015 ], Bias: [-1.2960185]\n",
      "Epoch 5800, Loss: 6.370067119598389, Weights: [0.42626214 0.5897964  1.0162938 ], Bias: [-1.2961019]\n",
      "Epoch 5900, Loss: 6.334267616271973, Weights: [0.42442596 0.58672315 1.0210826 ], Bias: [-1.2961854]\n",
      "Epoch 6000, Loss: 6.301384449005127, Weights: [0.42263913 0.5837994  1.0256772 ], Bias: [-1.2962688]\n",
      "Epoch 6100, Loss: 6.271160125732422, Weights: [0.42090064 0.5810185  1.0300853 ], Bias: [-1.2963523]\n",
      "Epoch 6200, Loss: 6.2433857917785645, Weights: [0.41920924 0.5783737  1.0343146 ], Bias: [-1.2964357]\n",
      "Epoch 6300, Loss: 6.217856407165527, Weights: [0.41756344 0.5758586  1.038373  ], Bias: [-1.2965192]\n",
      "Epoch 6400, Loss: 6.194375038146973, Weights: [0.415962   0.57346743 1.042267  ], Bias: [-1.2966026]\n",
      "Epoch 6500, Loss: 6.172796249389648, Weights: [0.41440356 0.5711943  1.0460038 ], Bias: [-1.296686]\n",
      "Epoch 6600, Loss: 6.1529412269592285, Weights: [0.41288725 0.56903404 1.0495898 ], Bias: [-1.2967811]\n",
      "Epoch 6700, Loss: 6.134677886962891, Weights: [0.4114114 0.566981  1.0530313], Bias: [-1.2968764]\n",
      "Epoch 6800, Loss: 6.117872714996338, Weights: [0.40997508 0.5650302  1.0563347 ], Bias: [-1.2969718]\n",
      "Epoch 6900, Loss: 6.102407455444336, Weights: [0.40857738 0.56317717 1.0595051 ], Bias: [-1.2970672]\n",
      "Epoch 7000, Loss: 6.088165283203125, Weights: [0.40721676 0.5614171  1.0625485 ], Bias: [-1.2971625]\n",
      "Epoch 7100, Loss: 6.075078010559082, Weights: [0.40589267 0.5597456  1.0654699 ], Bias: [-1.2972579]\n",
      "Epoch 7200, Loss: 6.062999725341797, Weights: [0.40460363 0.5581587  1.0682745 ], Bias: [-1.2973533]\n",
      "Epoch 7300, Loss: 6.051894664764404, Weights: [0.40334898 0.5566524  1.070967  ], Bias: [-1.2974486]\n",
      "Epoch 7400, Loss: 6.04164981842041, Weights: [0.40212783 0.555223   1.0735518 ], Bias: [-1.297544]\n",
      "Epoch 7500, Loss: 6.032232284545898, Weights: [0.400939   0.55386645 1.0760338 ], Bias: [-1.2976394]\n",
      "Epoch 7600, Loss: 6.023531913757324, Weights: [0.39978158 0.5525799  1.0784172 ], Bias: [-1.2977347]\n",
      "Epoch 7700, Loss: 6.015515327453613, Weights: [0.39865476 0.5513595  1.0807059 ], Bias: [-1.2978301]\n",
      "Epoch 7800, Loss: 6.0081305503845215, Weights: [0.39755788 0.5502026  1.0829037 ], Bias: [-1.2979255]\n",
      "Epoch 7900, Loss: 6.001314163208008, Weights: [0.39648968 0.54910564 1.085015  ], Bias: [-1.2980208]\n",
      "Epoch 8000, Loss: 5.995021820068359, Weights: [0.39544976 0.5480665  1.0870422 ], Bias: [-1.2981198]\n",
      "Epoch 8100, Loss: 5.989214897155762, Weights: [0.39443746 0.5470818  1.0889895 ], Bias: [-1.2982271]\n",
      "Epoch 8200, Loss: 5.983867168426514, Weights: [0.39345157 0.5461494  1.0908602 ], Bias: [-1.2983344]\n",
      "Epoch 8300, Loss: 5.978905200958252, Weights: [0.3924919 0.5452662 1.0926572], Bias: [-1.2984416]\n",
      "Epoch 8400, Loss: 5.974348068237305, Weights: [0.39155725 0.54443055 1.0943834 ], Bias: [-1.2985489]\n",
      "Epoch 8500, Loss: 5.970117092132568, Weights: [0.39064717 0.54363954 1.0960423 ], Bias: [-1.2986562]\n",
      "Epoch 8600, Loss: 5.966226100921631, Weights: [0.3897611  0.54289126 1.0976359 ], Bias: [-1.2987635]\n",
      "Epoch 8700, Loss: 5.962625503540039, Weights: [0.3888981 0.5421837 1.0991673], Bias: [-1.2988708]\n",
      "Epoch 8800, Loss: 5.9592766761779785, Weights: [0.38805744 0.5415145  1.1006395 ], Bias: [-1.2989781]\n",
      "Epoch 8900, Loss: 5.956187725067139, Weights: [0.38723895 0.54088235 1.102054  ], Bias: [-1.2990854]\n",
      "Epoch 9000, Loss: 5.9533467292785645, Weights: [0.38644198 0.5402852  1.1034135 ], Bias: [-1.2991927]\n",
      "Epoch 9100, Loss: 5.950701713562012, Weights: [0.3856654 0.5397215 1.1047204], Bias: [-1.2993]\n",
      "Epoch 9200, Loss: 5.948242664337158, Weights: [0.38490927 0.53918946 1.1059762 ], Bias: [-1.2994072]\n",
      "Epoch 9300, Loss: 5.945984363555908, Weights: [0.3841728  0.53868765 1.1071837 ], Bias: [-1.2995145]\n",
      "Epoch 9400, Loss: 5.94388484954834, Weights: [0.3834556  0.53821427 1.1083447 ], Bias: [-1.2996218]\n",
      "Epoch 9500, Loss: 5.941936492919922, Weights: [0.38275695 0.5377684  1.1094606 ], Bias: [-1.2997291]\n",
      "Epoch 9600, Loss: 5.940138339996338, Weights: [0.38207635 0.5373483  1.1105338 ], Bias: [-1.2998364]\n",
      "Epoch 9700, Loss: 5.938461780548096, Weights: [0.3814136  0.53695273 1.1115658 ], Bias: [-1.2999437]\n",
      "Epoch 9800, Loss: 5.936904430389404, Weights: [0.3807676 0.5365805 1.1125587], Bias: [-1.300051]\n",
      "Epoch 9900, Loss: 5.935461521148682, Weights: [0.3801387  0.53623044 1.1135134 ], Bias: [-1.3001583]\n",
      "Epoch 10000, Loss: 5.934134483337402, Weights: [0.37952572 0.5359018  1.1144315 ], Bias: [-1.3002656]\n",
      "Epoch 10100, Loss: 5.932878017425537, Weights: [0.3789291  0.53559285 1.1153146 ], Bias: [-1.3003728]\n",
      "Epoch 10200, Loss: 5.931723117828369, Weights: [0.37834743 0.5353035  1.1161638 ], Bias: [-1.3004801]\n",
      "Epoch 10300, Loss: 5.930647373199463, Weights: [0.3777808 0.5350316 1.1169814], Bias: [-1.3005874]\n",
      "Epoch 10400, Loss: 5.929645538330078, Weights: [0.37722936 0.5347766  1.1177679 ], Bias: [-1.3006947]\n",
      "Epoch 10500, Loss: 5.928717136383057, Weights: [0.3766918 0.5345382 1.1185244], Bias: [-1.300802]\n",
      "Epoch 10600, Loss: 5.927845001220703, Weights: [0.37616757 0.53431606 1.1192524 ], Bias: [-1.3009093]\n",
      "Epoch 10700, Loss: 5.92704963684082, Weights: [0.3756569  0.53410804 1.1199534 ], Bias: [-1.3010166]\n",
      "Epoch 10800, Loss: 5.9262919425964355, Weights: [0.3751596 0.5339134 1.120628 ], Bias: [-1.3011239]\n",
      "Epoch 10900, Loss: 5.9255852699279785, Weights: [0.37467533 0.533732   1.1212772 ], Bias: [-1.3012311]\n",
      "Epoch 11000, Loss: 5.924933433532715, Weights: [0.37420368 0.5335627  1.1219023 ], Bias: [-1.3013384]\n",
      "Epoch 11100, Loss: 5.924319267272949, Weights: [0.37374422 0.53340536 1.1225039 ], Bias: [-1.3014457]\n",
      "Epoch 11200, Loss: 5.9237565994262695, Weights: [0.3732965 0.5332595 1.123083 ], Bias: [-1.301553]\n",
      "Epoch 11300, Loss: 5.9232177734375, Weights: [0.37286013 0.53312415 1.1236405 ], Bias: [-1.3016603]\n",
      "Epoch 11400, Loss: 5.922723770141602, Weights: [0.37243482 0.53299856 1.1241779 ], Bias: [-1.3017676]\n",
      "Epoch 11500, Loss: 5.922247886657715, Weights: [0.3720202 0.5328827 1.1246955], Bias: [-1.3018749]\n",
      "Epoch 11600, Loss: 5.921824932098389, Weights: [0.37161613 0.5327765  1.1251932 ], Bias: [-1.3019822]\n",
      "Epoch 11700, Loss: 5.92141056060791, Weights: [0.37122166 0.5326786  1.1256737 ], Bias: [-1.3020895]\n",
      "Epoch 11800, Loss: 5.9210286140441895, Weights: [0.37083805 0.5325887  1.1261355 ], Bias: [-1.3021967]\n",
      "Epoch 11900, Loss: 5.920678615570068, Weights: [0.3704641 0.532506  1.1265812], Bias: [-1.302304]\n",
      "Epoch 12000, Loss: 5.920345306396484, Weights: [0.3701001 0.5324303 1.12701  ], Bias: [-1.3024149]\n",
      "Epoch 12100, Loss: 5.920020580291748, Weights: [0.36974558 0.5323617  1.127423  ], Bias: [-1.302534]\n",
      "Epoch 12200, Loss: 5.919723033905029, Weights: [0.3693996  0.53229946 1.1278217 ], Bias: [-1.3026532]\n",
      "Epoch 12300, Loss: 5.919439315795898, Weights: [0.36906177 0.5322437  1.1282059 ], Bias: [-1.3027724]\n",
      "Epoch 12400, Loss: 5.919188022613525, Weights: [0.3687331 0.5321932 1.128576 ], Bias: [-1.3028916]\n",
      "Epoch 12500, Loss: 5.918941020965576, Weights: [0.3684133  0.53214765 1.1289327 ], Bias: [-1.3030108]\n",
      "Epoch 12600, Loss: 5.918701171875, Weights: [0.36810157 0.5321071  1.1292768 ], Bias: [-1.30313]\n",
      "Epoch 12700, Loss: 5.918488025665283, Weights: [0.3677974 0.5320719 1.1296083], Bias: [-1.3032492]\n",
      "Epoch 12800, Loss: 5.918279647827148, Weights: [0.3675006 0.5320414 1.1299278], Bias: [-1.3033684]\n",
      "Epoch 12900, Loss: 5.918085098266602, Weights: [0.36721176 0.5320143  1.1302365 ], Bias: [-1.3034877]\n",
      "Epoch 13000, Loss: 5.917910575866699, Weights: [0.36693034 0.53199095 1.1305344 ], Bias: [-1.3036069]\n",
      "Epoch 13100, Loss: 5.917744159698486, Weights: [0.36665556 0.53197277 1.1308206 ], Bias: [-1.3037261]\n",
      "Epoch 13200, Loss: 5.9175825119018555, Weights: [0.36638832 0.5319573  1.131097  ], Bias: [-1.3038453]\n",
      "Epoch 13300, Loss: 5.917424201965332, Weights: [0.36612794 0.5319448  1.1313635 ], Bias: [-1.3039645]\n",
      "Epoch 13400, Loss: 5.9172821044921875, Weights: [0.36587375 0.5319352  1.1316214 ], Bias: [-1.3040837]\n",
      "Epoch 13500, Loss: 5.917139053344727, Weights: [0.36562505 0.5319292  1.1318704 ], Bias: [-1.3042029]\n",
      "Epoch 13600, Loss: 5.917020797729492, Weights: [0.36538398 0.53192556 1.1321098 ], Bias: [-1.3043221]\n",
      "Epoch 13700, Loss: 5.916888236999512, Weights: [0.365149   0.53192466 1.1323403 ], Bias: [-1.3044413]\n",
      "Epoch 13800, Loss: 5.91677188873291, Weights: [0.36491907 0.531925   1.132565  ], Bias: [-1.3045605]\n",
      "Epoch 13900, Loss: 5.916654586791992, Weights: [0.36469543 0.53192896 1.1327798 ], Bias: [-1.3046798]\n",
      "Epoch 14000, Loss: 5.916552543640137, Weights: [0.36447772 0.53193456 1.1329873 ], Bias: [-1.304799]\n",
      "Epoch 14100, Loss: 5.916456699371338, Weights: [0.36426508 0.5319408  1.1331893 ], Bias: [-1.3049182]\n",
      "Epoch 14200, Loss: 5.916378021240234, Weights: [0.36405835 0.53195125 1.1333816 ], Bias: [-1.3050374]\n",
      "Epoch 14300, Loss: 5.916274547576904, Weights: [0.3638555 0.5319618 1.1335698], Bias: [-1.3051566]\n",
      "Epoch 14400, Loss: 5.916193008422852, Weights: [0.36365965 0.531974   1.1337496 ], Bias: [-1.3052758]\n",
      "Epoch 14500, Loss: 5.916110992431641, Weights: [0.36346722 0.53198636 1.1339259 ], Bias: [-1.305395]\n",
      "Epoch 14600, Loss: 5.9160284996032715, Weights: [0.36328056 0.5320022  1.1340934 ], Bias: [-1.3055142]\n",
      "Epoch 14700, Loss: 5.915950775146484, Weights: [0.3630975 0.5320178 1.1342574], Bias: [-1.3056334]\n",
      "Epoch 14800, Loss: 5.9158854484558105, Weights: [0.3629208 0.5320357 1.1344131], Bias: [-1.3057526]\n",
      "Epoch 14900, Loss: 5.915821552276611, Weights: [0.36274624 0.5320536  1.1345668 ], Bias: [-1.3058718]\n",
      "Epoch 15000, Loss: 5.9157609939575195, Weights: [0.3625782  0.53207266 1.1347127 ], Bias: [-1.305991]\n",
      "Epoch 15100, Loss: 5.9156951904296875, Weights: [0.36241382 0.53209144 1.1348557 ], Bias: [-1.3061103]\n",
      "Epoch 15200, Loss: 5.915650844573975, Weights: [0.3622532 0.5321118 1.1349933], Bias: [-1.3062295]\n",
      "Epoch 15300, Loss: 5.915583610534668, Weights: [0.36209697 0.5321345  1.1351246 ], Bias: [-1.3063487]\n",
      "Epoch 15400, Loss: 5.915531635284424, Weights: [0.36194333 0.5321552  1.1352551 ], Bias: [-1.3064679]\n",
      "Epoch 15500, Loss: 5.915470123291016, Weights: [0.361795   0.53217876 1.1353774 ], Bias: [-1.3065871]\n",
      "Epoch 15600, Loss: 5.915421962738037, Weights: [0.36165005 0.5322026  1.1354966 ], Bias: [-1.3067063]\n",
      "Epoch 15700, Loss: 5.9153876304626465, Weights: [0.3615074 0.5322256 1.1356142], Bias: [-1.3068255]\n",
      "Epoch 15800, Loss: 5.915333271026611, Weights: [0.36137035 0.53224957 1.1357253 ], Bias: [-1.3069447]\n",
      "Epoch 15900, Loss: 5.915294170379639, Weights: [0.36123702 0.53227365 1.1358328 ], Bias: [-1.3070639]\n",
      "Epoch 16000, Loss: 5.915244102478027, Weights: [0.36110482 0.5322975  1.1359395 ], Bias: [-1.3071831]\n",
      "Epoch 16100, Loss: 5.9152116775512695, Weights: [0.36097696 0.53232163 1.1360414 ], Bias: [-1.3073024]\n",
      "Epoch 16200, Loss: 5.915166854858398, Weights: [0.36085412 0.5323463  1.1361381 ], Bias: [-1.3074216]\n",
      "Epoch 16300, Loss: 5.9151153564453125, Weights: [0.36073303 0.5323708  1.1362334 ], Bias: [-1.3075408]\n",
      "Epoch 16400, Loss: 5.915087699890137, Weights: [0.36061358 0.53239465 1.1363277 ], Bias: [-1.30766]\n",
      "Epoch 16500, Loss: 5.915056228637695, Weights: [0.36049855 0.53241897 1.136417  ], Bias: [-1.3077792]\n",
      "Epoch 16600, Loss: 5.915012836456299, Weights: [0.36038762 0.5324444  1.1365013 ], Bias: [-1.3078984]\n",
      "Epoch 16700, Loss: 5.914978504180908, Weights: [0.36027852 0.53246915 1.1365848 ], Bias: [-1.3080176]\n",
      "Epoch 16800, Loss: 5.914949893951416, Weights: [0.36017063 0.53249305 1.1366678 ], Bias: [-1.3081368]\n",
      "Epoch 16900, Loss: 5.914916515350342, Weights: [0.36006615 0.53251714 1.1367471 ], Bias: [-1.308256]\n",
      "Epoch 17000, Loss: 5.914890289306641, Weights: [0.35996562 0.5325427  1.1368213 ], Bias: [-1.3083752]\n",
      "Epoch 17100, Loss: 5.914860248565674, Weights: [0.3598677  0.53256786 1.1368932 ], Bias: [-1.3084944]\n",
      "Epoch 17200, Loss: 5.914831161499023, Weights: [0.3597714 0.5325921 1.1369647], Bias: [-1.3086137]\n",
      "Epoch 17300, Loss: 5.9148030281066895, Weights: [0.3596758  0.53261596 1.1370357 ], Bias: [-1.3087329]\n",
      "Epoch 17400, Loss: 5.914773464202881, Weights: [0.35958332 0.5326399  1.1371036 ], Bias: [-1.3088521]\n",
      "Epoch 17500, Loss: 5.914761543273926, Weights: [0.35949463 0.5326642  1.1371675 ], Bias: [-1.3089713]\n",
      "Epoch 17600, Loss: 5.914731502532959, Weights: [0.3594087 0.5326891 1.1372279], Bias: [-1.3090905]\n",
      "Epoch 17700, Loss: 5.914703369140625, Weights: [0.3593246 0.5327133 1.1372876], Bias: [-1.3092097]\n",
      "Epoch 17800, Loss: 5.914680004119873, Weights: [0.35924095 0.53273714 1.1373472 ], Bias: [-1.3093289]\n",
      "Epoch 17900, Loss: 5.914648056030273, Weights: [0.35915807 0.5327607  1.1374062 ], Bias: [-1.3094481]\n",
      "Epoch 18000, Loss: 5.9146246910095215, Weights: [0.35907805 0.5327845  1.137462  ], Bias: [-1.3095673]\n",
      "Epoch 18100, Loss: 5.914602279663086, Weights: [0.35900116 0.53280836 1.1375148 ], Bias: [-1.3096865]\n",
      "Epoch 18200, Loss: 5.914579391479492, Weights: [0.3589269 0.5328324 1.1375648], Bias: [-1.3098058]\n",
      "Epoch 18300, Loss: 5.914545059204102, Weights: [0.35885486 0.5328563  1.1376129 ], Bias: [-1.309925]\n",
      "Epoch 18400, Loss: 5.9145331382751465, Weights: [0.35878345 0.5328801  1.1376606 ], Bias: [-1.3100442]\n",
      "Epoch 18500, Loss: 5.914514541625977, Weights: [0.35871282 0.53290313 1.1377083 ], Bias: [-1.3101634]\n",
      "Epoch 18600, Loss: 5.914488315582275, Weights: [0.35864413 0.5329241  1.137756  ], Bias: [-1.3102826]\n",
      "Epoch 18700, Loss: 5.9144673347473145, Weights: [0.35857654 0.5329439  1.1378037 ], Bias: [-1.3104018]\n",
      "Epoch 18800, Loss: 5.914456844329834, Weights: [0.35851136 0.53296447 1.1378484 ], Bias: [-1.310521]\n",
      "Epoch 18900, Loss: 5.914435863494873, Weights: [0.35844833 0.53298634 1.1378896 ], Bias: [-1.3106402]\n",
      "Epoch 19000, Loss: 5.9144206047058105, Weights: [0.35838756 0.53300947 1.1379275 ], Bias: [-1.3107594]\n",
      "Epoch 19100, Loss: 5.914393424987793, Weights: [0.35832822 0.53303236 1.1379642 ], Bias: [-1.3108786]\n",
      "Epoch 19200, Loss: 5.914374828338623, Weights: [0.3582709 0.5330539 1.1380002], Bias: [-1.3109978]\n",
      "Epoch 19300, Loss: 5.914352893829346, Weights: [0.35821474 0.53307456 1.138036  ], Bias: [-1.311117]\n",
      "Epoch 19400, Loss: 5.914337158203125, Weights: [0.3581603  0.53309375 1.1380718 ], Bias: [-1.3112363]\n",
      "Epoch 19500, Loss: 5.914315223693848, Weights: [0.3581066 0.533112  1.1381075], Bias: [-1.3113555]\n",
      "Epoch 19600, Loss: 5.914288520812988, Weights: [0.35805336 0.53312993 1.1381433 ], Bias: [-1.3114747]\n",
      "Epoch 19700, Loss: 5.914278507232666, Weights: [0.3580002 0.5331478 1.138179 ], Bias: [-1.3115939]\n",
      "Epoch 19800, Loss: 5.914263725280762, Weights: [0.3579485 0.5331657 1.138213 ], Bias: [-1.3117131]\n",
      "Epoch 19900, Loss: 5.914239406585693, Weights: [0.3578986 0.5331836 1.1382456], Bias: [-1.3118323]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.00001)\n",
    "\n",
    "for epoch in range(20000):\n",
    "    with tf.GradientTape() as tape:    \n",
    "        loss = loos_fn(model, x_train, y_train)\n",
    "\n",
    "        gradients = tape.gradient(loss, [model.w, model.b])\n",
    "        optimizer.apply_gradients(zip(gradients, [model.w, model.b]))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.numpy()}, Weights: {model.w.numpy().flatten()}, Bias: {model.b.numpy().flatten()}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "41600446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values for 6.0, 7.0, 8.0: [152.83798 165.87602 168.51343]\n"
     ]
    }
   ],
   "source": [
    "prediected = model(np.array([[73.0, 80.0, 75.0], [98.0, 77.0, 80.0], [86.0, 90.0, 80.0]], dtype=np.float32))\n",
    "print(\"Predicted values for 6.0, 7.0, 8.0:\", prediected.numpy().flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
